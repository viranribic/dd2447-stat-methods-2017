x[,1] = rnorm(N,phi*x0,sigma)
xss[,1] = rnorm(N,phi*x0,sigma)
beta = 0.5
y = c(2)
xss = array(0,c(T,N))
wss = array(0,c(T,N))
x0 = rnorm(N, 0, sqrt(sigma^2/(1-phi^2)))
xss[,1] = rnorm(N, phi*x0, sigma)
wss[,1] = pnorm(y[1], 0, (beta^2)*exp(xss[,1]))
View(wss)
T = 500
N = 100
phi = 0.98
sigma = 0.16
beta = 0.5
# At each time t, we sample N samples, and therefore we have N weights
xss = array(0,c(T,N))
wss = array(0,c(T,N))
# Time t = 1
x0 = rnorm(N, 0, sqrt(sigma^2/(1-phi^2)))
xss[1,] = rnorm(N, phi*x0, sigma)
length(rnorm(N, phi*x0, sigma))
length(pnorm(y[1], 0, (beta^2)*exp(xss[1,])) )
length(rnorm(1, 0, (beta^2)*exp(xss[1,])))
length(exp(xss[1,])
)
1:2:0.1
1:0.1:2
estimateLikelihood = function(beta){
set.seed(12254)
T = 500
N = 100
phi = 0.98
sigma = 0.16
# At each time t, we sample N samples, and therefore we have N weights
xss = array(0,c(T,N))
wss = array(0,c(T,N))
# Time t = 1
x0 = rnorm(N, 0, sqrt(sigma^2/(1-phi^2)))
xss[1,] = rnorm(N, phi*x0, sigma)
# Supposing to have y vector of length T, the data, the weight is given by
# the probability of that datapoint given the sample
# wss[1,] = pnorm(y[1], 0, (beta^2)*exp(xss[1,]))
for (i in 1:N)
wss[1,i] = rnorm(1, 0, (beta^2)*exp(xss[1,i]))
for (t in 2:T){
xss[t,] = rnorm(N, phi*x[t-1,], sigma)
#wss[t,] = pnorm(y[t], 0, (beta^2)*exp(xss[t,]))
for (i in 1:N)
wss[t,i] = rnorm(1, 0, (beta^2)*exp(xss[t,i]))
}
l = 0
for (t in 1:T)
l = l + sum(wss[t,]) - log(N)
return(l)
}
beta = seq(0, 2, length.out = 10)
likelihood = NULL
for (k in 1:10){
l = NULL
for (b in beta)
l = c(l, estimateLikelihood(b))
likelihood = rbind(likelihood, l)
}
estimateLikelihood = function(beta){
set.seed(12254)
T = 500
N = 100
phi = 0.98
sigma = 0.16
# At each time t, we sample N samples, and therefore we have N weights
x = array(0,c(T,N))
w = array(0,c(T,N))
# Time t = 1
x0 = rnorm(N, 0, sqrt(sigma^2/(1-phi^2)))
x[1,] = rnorm(N, phi*x0, sigma)
# Supposing to have y vector of length T, the data, the weight is given by
# the probability of that datapoint given the sample
# w[1,] = pnorm(y[1], 0, (beta^2)*exp(x[1,]))
for (i in 1:N)
w[1,i] = rnorm(1, 0, (beta^2)*exp(x[1,i]))
for (t in 2:T){
x[t,] = rnorm(N, phi*x[t-1,], sigma)
#wss[t,] = pnorm(y[t], 0, (beta^2)*exp(xss[t,]))
for (i in 1:N)
w[t,i] = rnorm(1, 0, (beta^2)*exp(x[t,i]))
}
l = 0
for (t in 1:T)
l = l + sum(w[t,]) - log(N)
return(l)
}
beta = seq(0, 2, length.out = 10)
likelihood = NULL
for (k in 1:10){
l = NULL
for (b in beta)
l = c(l, estimateLikelihood(b))
likelihood = rbind(likelihood, l)
}
View(likelihood)
estimateLikelihood = function(beta){
# set.seed(12254)
T = 500
N = 100
phi = 0.98
sigma = 0.16
# At each time t, we sample N samples, and therefore we have N weights
x = array(0,c(T,N))
w = array(0,c(T,N))
# Time t = 1
x0 = rnorm(N, 0, sqrt(sigma^2/(1-phi^2)))
x[1,] = rnorm(N, phi*x0, sigma)
# Supposing to have y vector of length T, the data, the weight is given by
# the probability of that datapoint given the sample
# w[1,] = pnorm(y[1], 0, (beta^2)*exp(x[1,]))
for (i in 1:N)
w[1,i] = rnorm(1, 0, (beta^2)*exp(x[1,i]))
for (t in 2:T){
x[t,] = rnorm(N, phi*x[t-1,], sigma)
#wss[t,] = pnorm(y[t], 0, (beta^2)*exp(xss[t,]))
for (i in 1:N)
w[t,i] = rnorm(1, 0, (beta^2)*exp(x[t,i]))
}
l = 0
for (t in 1:T)
l = l + sum(w[t,]) - log(N)
return(l)
}
beta = seq(0, 2, length.out = 10)
likelihood = NULL
for (k in 1:10){
l = NULL
for (b in beta)
l = c(l, estimateLikelihood(b))
likelihood = rbind(likelihood, l)
}
View(likelihood)
?boxplot
?boxplot.matrix
beta = seq(0, 2, length.out = 20)
likelihood = NULL
for (k in 1:10){
l = NULL
for (b in beta)
l = c(l, estimateLikelihood(b))
likelihood = rbind(likelihood, l)
}
boxplot.matrix(likelihood, use.cols=FALSE)
boxplot.matrix(likelihood)
####################################################################
# SEQUENTIAL IMPORTANCE SAMPLING
####################################################################
estimateLikelihood = function(beta){
# set.seed(12254)
T = 500
N = 100
phi = 0.98
sigma = 0.16
# At each time t, we sample N samples, and therefore we have N weights
x = array(0,c(T,N))
w = array(0,c(T,N))
# Time t = 1
x0 = rnorm(N, 0, sqrt(sigma^2/(1-phi^2)))
# Supposing to have y vector of length T, the data, the weight is given by
# the probability of that datapoint given the sample
# w[1,] = pnorm(y[1], 0, (beta^2)*exp(x[1,]))
for (i in 1:N)
x[1,i] = rnorm(1, phi*x0[i], sigma)
w[1,i] = rnorm(1, 0, (beta^2)*exp(x[1,i]))
for (t in 2:T){
for (i in 1:N){
x[t,i] = rnorm(1, phi*x[t-1,i], sigma)
#wss[t,] = pnorm(y[t], 0, (beta^2)*exp(xss[t,]))
w[t,i] = rnorm(1, 0, (beta^2)*exp(x[t,i]))
}
}
l = 0
for (t in 1:T)
l = l + sum(w[t,]) - log(N)
return(l)
}
beta = seq(0, 2, length.out = 20)
likelihood = NULL
for (k in 1:10){
l = NULL
for (b in beta)
l = c(l, estimateLikelihood(b))
likelihood = rbind(likelihood, l)
}
boxplot.matrix(likelihood)
chooseK = function(path){
E = read.csv(path)
col1 = E[,1]
col2 = E[,2]
max_ids = c(max(col1), max(col2))
A = matrix(0, nrow = max_ids[1], ncol = max_ids[2])
for (i in 1:length(col1)){
A[col1[i],col2[i]] = 1
}
invD = diag(nrow = dim(A)[1])
D = diag(nrow = dim(A)[1])
for (i in 1:dim(A)[1]){
D[i,i] = sum(A[i,])
invD[i,i] = 1/D[i,i]
}
L = sqrt(invD)%*%A%*%sqrt(invD)
eigenvalues = sort(eigen(D-A)$values)
# The number of connected components in the graph is the dimension
# of the nullspace of the Laplacian and the algebraic multiplicity of the 0 eigenvalue.
# For this reason, to choose k, it's important to plot the eigenvalues of L
# The plot of the eigenvalues of the symmetric normalized Laplacian is used to confirm
# the conclusions drawn by the other plot.
# Also, to identify k clusters,  the eigenvalue spectrum of L  must have a gap,
x11()
par(mfrow=c(1,2))
plot(1:length(eigenvalues), eigenvalues, ylab = "Eigenvalues of Laplacian")
plot(1:length(eigenvalues), sort(eigen(L)$values), ylab = "Eigenvalues of symmetric normalized Laplacian")
# If good clusters can be identified, then the  Laplacian L  is approximately block-diagonal,
# with each block defining a cluster.
x11()
image(1:dim(L)[1],1:dim(L)[2],as.matrix(L), main='Sparsity Pattern', asp=1, xlab='i', ylab='j' )
return(eigen(L)$vectors)
}
# This function actually computes the clusters through the transformation of the data
spectralClustering = function(eigenvectors, k){
Y = eigenvectors[,1:k]
clustering = kmeans(Y, centers = k)
# Each row of the original matrix A (each node) is assigned to a cluster
# clustering$cluster is a vector of n elements, each element correspond to a node
return(clustering$cluster)
}
V1 = chooseK("C:/Users/Vale/Dropbox/Poli/Erasmus/KTH/Data Mining/Assignments/Lab 4 - Graph Spectra/example1.dat")
V2 = chooseK("C:/Users/Vale/Dropbox/Poli/Erasmus/KTH/Data Mining/Assignments/Lab 4 - Graph Spectra/example2.dat")
setwd("C:/Users/Vale/Dropbox/Poli/Erasmus/KTH/Statitical Methods for Applied Computer Science/DD2447_2017/task1")
source("task1_functions.R")
# Question 1
#-------------------------------------------------------------------
y = read.table("output.txt")
beta = seq(0.1, 2, length.out = 20)
likelihood = NULL
for (k in 1:10){
l = NULL
# l = sapply(beta, estimateLikelihood, y=y)
for (b in beta){
temp = estimateLikelihood(b,y)
while (temp[2]<100){
temp = estimateLikelihood(b,y)
}
l = c(l, temp[1])
}
likelihood = rbind(likelihood, l)
}
x11()
boxplot.matrix(likelihood, xlab = "Beta values", ylab = "Log-likelihood", main = "SMC without resampling", names = round(beta, digit=2))
setwd("C:/Users/Vale/Dropbox/Poli/Erasmus/KTH/Statitical Methods for Applied Computer Science/DD2447_2017/task1")
source("task1_functions.R")
setwd("C:/Users/Vale/Dropbox/Poli/Erasmus/KTH/Statitical Methods for Applied Computer Science/DD2447_2017/task1")
source("task1_functions.R")
y = read.table("output.txt")
beta = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1)
likelihood = NULL
for (k in 1:10){
l = NULL
# l = sapply(beta, estimateLikelihood, y=y)
for (b in beta){
temp = estimateLikelihood(b,y)
while (temp[2]<100){
temp = estimateLikelihood(b,y)
}
l = c(l, temp[1])
}
likelihood = rbind(likelihood, l)
}
x11()
boxplot.matrix(likelihood, xlab = "Beta values", ylab = "Log-likelihood", main = "SMC without resampling", names = round(beta, digit=2))
boxplot.matrix(likelihood, xlab = "Beta values", ylab = "Log-likelihood", main = "SMC without resampling", names = round(beta, digit=2), ylim=c(-250, max(likelihood)))
x11()
boxplot.matrix(likelihood, xlab = "Beta values", ylab = "Log-likelihood", main = "SMC without resampling", names = round(beta, digit=2), ylim=c(-250, max(likelihood)))
likelihood = NULL
for (k in 1:10){
l = NULL
l = sapply(beta, estimateLikelihoodResampl, y=y)
likelihood = rbind(likelihood, l)
}
likelihood = NULL
for (k in 1:10){
l = NULL
# l = sapply(beta, estimateLikelihood, y=y)
for (b in beta){
temp = estimateLikelihoodResampl(b,y)
while (temp[2]<100){
temp = estimateLikelihoodResampl(b,y)
}
l = c(l, temp[1])
}
likelihood = rbind(likelihood, l)
}
x11()
boxplot.matrix(likelihood, xlab = "Beta values", ylab = "Log-likelihood", main = "SMC with resampling", names = round(beta, digit=2))
setwd("C:/Users/Vale/Dropbox/Poli/Erasmus/KTH/Statitical Methods for Applied Computer Science/DD2447_2017/task1")
source("task1_functions.R")
beta = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1)
likelihood = NULL
for (k in 1:10){
l = NULL
# l = sapply(beta, estimateLikelihood, y=y)
for (b in beta){
temp = estimateLikelihood(b,y)
while (temp[2]<100){
temp = estimateLikelihood(b,y)
}
l = c(l, temp[1])
}
likelihood = rbind(likelihood, l)
}
x11()
boxplot.matrix(likelihood, xlab = "Beta values", ylab = "Log-likelihood", main = "SMC without resampling", names = round(beta, digit=2), ylim=c(-250, max(likelihood)))
boxplot.matrix(likelihood, xlab = "Beta values", ylab = "Log-likelihood", main = "SMC without resampling", names = round(beta, digit=2)))
boxplot.matrix(likelihood, xlab = "Beta values", ylab = "Log-likelihood", main = "SMC without resampling", names = round(beta, digit=2))
beta = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1)
likelihood = NULL
for (k in 1:10){
l = NULL
# l = sapply(beta, estimateLikelihood, y=y)
for (b in beta){
temp = estimateLikelihoodResampl(b,y)
while (temp[2]<100){
temp = estimateLikelihoodResampl(b,y)
}
l = c(l, temp[1])
}
likelihood = rbind(likelihood, l)
}
x11()
boxplot.matrix(likelihood, xlab = "Beta values", ylab = "Log-likelihood", main = "SMC with resampling", names = round(beta, digit=2))
source("task5_functions.R")
setwd("C:/Users/Vale/Dropbox/Poli/Erasmus/KTH/Statitical Methods for Applied Computer Science/DD2447_2017/task5")
source("task5_functions.R")
library(invgamma)
library(pscl)
library(ggplot2)
T = 500
N = 10000
phi = 0.951934
X = NULL        # will have dimension TXN
sigma2 = NULL   # will have dimension 1xN
beta2 = NULL    # will have dimension 1xN
y = read.table("output.txt")
# Initialization of sigma2 and X:
# sigma is initialized to its correct value
# X is the matrix representing the results of each step of SMC
sigma = 0.1315
X_smc = smc_tot(sigma0, phi, T)
sigma0 = 0.1315
X_smc = smc_tot(sigma0, phi, T)
source("task5_functions.R")
T = 500
N = 10000
phi = 0.951934
X = NULL        # will have dimension TXN
sigma2 = NULL   # will have dimension 1xN
beta2 = NULL    # will have dimension 1xN
sigma0 = 0.1315
X_smc = smc_tot(sigma0, phi, T)
N = 100
for (i in 1:N){
sigma = update_sigma2(X_smc[T+1,], phi, T)
sigma2 = c(sigma2, sigma)
x = smc(X_smc, i, sigma, phi, T)
X_smc[,i] = x
beta = update_beta2(x, y[,1], T)
beta2 = c(beta2, beta)
}
i=1
X_smc[,i]
X_smc[,i] = x
sigma0 = 0.1315
X_smc = smc_tot(sigma0, phi, T)
# Gibbs sampling
for (i in 1:N){
sigma = update_sigma2(X_smc[T+1,], phi, T)
sigma2 = c(sigma2, sigma)
x = smc(X_smc, i, sigma, phi, T)
beta = update_beta2(x, y[,1], T)
beta2 = c(beta2, beta)
}
i=1
T = 500
N = 100
phi = 0.951934
X = NULL        # will have dimension TXN
sigma2 = NULL   # will have dimension 1xN
beta2 = NULL    # will have dimension 1xN
y = read.table("output.txt")
# Initialization of sigma2 and X:
# sigma is initialized to its correct value
# X is the matrix representing the results of each step of SMC
sigma0 = 0.1315
X_smc = smc_tot(sigma0, phi, T)
sigma = update_sigma2(X_smc[T+1,], phi, T)
View(X_smc)
sigma0 = 0.1315
for (i in 1:N){
sigma = update_sigma2(X_smc[,i], phi, T)
sigma2 = c(sigma2, sigma)
x = smc(X_smc, i, sigma, phi, T)
X_smc[,i] = x
beta = update_beta2(x, y[,1], T)
beta2 = c(beta2, beta)
}
setwd("C:/Users/Vale/Dropbox/Poli/Erasmus/KTH/Statitical Methods for Applied Computer Science/DD2447_2017/task1")
source("task1_functions.R")
beta = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1)
likelihood = NULL
for (k in 1:10){
l = NULL
# l = sapply(beta, estimateLikelihood, y=y)
for (b in beta){
temp = estimateLikelihood(b,y)
while (temp[2]<100){
temp = estimateLikelihood(b,y)
}
l = c(l, temp[1])
}
likelihood = rbind(likelihood, l)
}
x11()
boxplot.matrix(likelihood, xlab = "Beta values", ylab = "Log-likelihood", main = "SMC without resampling", names = round(beta, digit=2))
beta = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1)
likelihood = NULL
for (k in 1:10){
l = NULL
# l = sapply(beta, estimateLikelihood, y=y)
for (b in beta){
temp = estimateLikelihood(b,y)
while (temp[2]<100){
temp = estimateLikelihood(b,y)
}
l = c(l, temp[1])
}
likelihood = rbind(likelihood, l)
}
x11()
boxplot.matrix(likelihood, xlab = "Beta values", ylab = "Log-likelihood", main = "SMC without resampling", names = round(beta, digit=2))
setwd("C:/Users/Vale/Dropbox/Poli/Erasmus/KTH/Statitical Methods for Applied Computer Science/DD2447_2017/task1")
source("task1_functions.R")
y = read.table("output.txt")
beta = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2)
likelihood = NULL
for (k in 1:10){
l = NULL
# l = sapply(beta, estimateLikelihood, y=y)
for (b in beta){
temp = estimateLikelihood(b,y)
while (temp[2]<100){
temp = estimateLikelihood(b,y)
}
l = c(l, temp[1])
}
likelihood = rbind(likelihood, l)
}
?seq
beta = seq(0.1, 2, length.out = 10)
likelihood = NULL
for (k in 1:10){
l = NULL
# l = sapply(beta, estimateLikelihood, y=y)
for (b in beta){
temp = estimateLikelihood(b,y)
while (temp[2]<100){
temp = estimateLikelihood(b,y)
}
l = c(l, temp[1])
}
likelihood = rbind(likelihood, l)
}
x11()
boxplot.matrix(likelihood, xlab = "Beta values", ylab = "Log-likelihood", main = "SMC without resampling", names = round(beta, digit=2))
likelihood = NULL
for (k in 1:10){
l = NULL
# l = sapply(beta, estimateLikelihood, y=y)
for (b in beta){
temp = estimateLikelihoodResampl(b,y)
while (temp[2]<100){
temp = estimateLikelihoodResampl(b,y)
}
l = c(l, temp[1])
}
likelihood = rbind(likelihood, l)
}
setwd("C:/Users/Vale/Dropbox/Poli/Erasmus/KTH/Statitical Methods for Applied Computer Science/DD2447_2017/task1")
source("task1_functions.R")
y = read.table("output.txt")
# beta = c(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2)
beta = seq(0.1, 2, length.out = 20)
likelihood = NULL
for (k in 1:10){
l = NULL
# l = sapply(beta, estimateLikelihood, y=y)
for (b in beta){
temp = estimateLikelihood(b,y)
while (temp[2]<100){
temp = estimateLikelihood(b,y)
}
l = c(l, temp[1])
}
likelihood = rbind(likelihood, l)
}
x11()
boxplot.matrix(likelihood, xlab = "Beta values", ylab = "Log-likelihood", main = "SMC without resampling", names = round(beta, digit=2))
